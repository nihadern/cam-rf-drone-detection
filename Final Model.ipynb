{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neccesary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_rf = np.load(\"data/backgroundRF_resampled.npy\")\n",
    "drone_rf = np.load(\"data/droneRF_resampled.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_imgs = np.load(\"data/images_background_resized.npy\", allow_pickle=True)\n",
    "drone_imgs = np.load(\"data/images_drone_resized.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drone_imgs = np.array(drone_imgs, dtype=np.float64)\n",
    "background_imgs = np.array(background_imgs, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drone_imgs /= 255\n",
    "background_imgs /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(drone_imgs)\n",
    "random.shuffle(background_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ test split and data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/NewUsersDir/mohammed/nkalathi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "images_data = np.append(background_imgs[0:len(background_rf)], \n",
    "                   drone_imgs[0:len(drone_rf)], axis=0)\n",
    "images_resized = []\n",
    "for i, img in enumerate(images_data):\n",
    "    images_resized.append(cv2.resize(img, (256, 256), \n",
    "                                interpolation = cv2.INTER_AREA))\n",
    "    images_resized[i] = np.expand_dims(images_resized[i], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized = np.array(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(images_resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([0 for i in enumerate(background_rf)] + [1 for i in enumerate(drone_rf)])\n",
    "X = np.append(background_rf,drone_rf,axis=0)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(rf_data,images):\n",
    "    low = []\n",
    "    high = []\n",
    "    images = images\n",
    "    for i, rf in enumerate(rf_data):\n",
    "        low.append(rf[0].astype(np.float16).flatten())\n",
    "        high.append(rf[1].astype(np.float16).flatten())\n",
    "    low = np.array(low)\n",
    "    high = np.array(high)\n",
    "    images = np.array(images)\n",
    "    return [low, high, images]\n",
    "X = format_data(X, images_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(x_data):\n",
    "    data = []\n",
    "    for i in range(len(x_data[0])):\n",
    "        data.append((x_data[0][i], x_data[1][i], x_data[2][i]))\n",
    "    return data\n",
    "def unjoin_data(x_data):\n",
    "    low = []\n",
    "    high = []\n",
    "    imags = []\n",
    "    for x in x_data:\n",
    "        low.append(x[0])\n",
    "        high.append(x[1])\n",
    "        imags.append(x[2])\n",
    "    low = np.array(low)\n",
    "    high = np.array(high)\n",
    "    imags = np.array(imags)\n",
    "    return [low, high, imags] \n",
    "X = join_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = unjoin_data(x_test)\n",
    "x_train = unjoin_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 4882)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv2d_4_input (InputLayer)     [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 254, 254, 32) 320         conv2d_4_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 254, 254, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 252, 252, 32) 9248        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 252, 252, 32) 128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 126, 126, 32) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 126, 126, 32) 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 124, 124, 64) 18496       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 124, 124, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 124, 124, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 122, 122, 128 73856       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 122, 122, 128 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 61, 61, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 61, 61, 128)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 476288)       0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 512)          243859968   flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4882)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 4882)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 512)          2048        dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 100)          488300      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          488300      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 50)           5050        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 50)           5050        dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          65664       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 228)          0           dense_9[0][0]                    \n",
      "                                                                 dense_11[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 10)           2290        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            11          dense_14[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 245,019,625\n",
      "Trainable params: 245,018,089\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, concatenate\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "\n",
    "\n",
    "\n",
    "img_shape = (256,256,1)\n",
    "# define three sets of inputs\n",
    "low_rf  = Input(shape=(x_train[0].shape[1],))\n",
    "high_rf = Input(shape=(x_train[1].shape[1],))\n",
    "\n",
    "# the first branch operates on the first input\n",
    "x1 = Dense(100 , activation=\"relu\")(low_rf)\n",
    "x1 = Dense(50, activation=\"relu\")(x1)\n",
    "x1 = Model(inputs=low_rf, outputs=x1)\n",
    "\n",
    "# the second branch operates on the second input\n",
    "x2 = Dense(100 , activation=\"relu\")(high_rf)\n",
    "x2 = Dense(50, activation=\"relu\")(x2)\n",
    "x2 = Model(inputs=high_rf, outputs=x2)\n",
    "\n",
    "# third branch for images \n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=img_shape))\n",
    "cnn.add(BatchNormalization())\n",
    "\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(512, activation='relu'))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.5))\n",
    "\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x1.output, x2.output, cnn.output])\n",
    "# combined \n",
    "\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(10, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(inputs=[x1.input, x2.input, cnn.input], outputs=z)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer ='adam' , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"training_checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 24 samples\n",
      "Epoch 1/2\n",
      "56/56 [==============================] - 4s 79ms/sample - loss: 0.6415 - accuracy: 0.5000 - val_loss: 0.6047 - val_accuracy: 0.6667\n",
      "Epoch 2/2\n",
      "56/56 [==============================] - 4s 79ms/sample - loss: 0.3490 - accuracy: 0.8929 - val_loss: 0.5240 - val_accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"training_checkpoint.h5\")\n",
    "batch_size =1\n",
    "epochs = 2\n",
    "out = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate on fold #1\n",
      "Training accurracy: 0.953125\n",
      "Validation accurracy: 1.0\n",
      "{'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 16}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 16}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 16}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-50bb40c55432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprecision_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "X = np.array(X)\n",
    "kf = KFold(n_splits=5)\n",
    "batch_size =1\n",
    "epochs = 2\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "train_acc_sum = 0\n",
    "val_acc_sum =0 \n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum  = 0 \n",
    "sensitivity_sum = 0\n",
    "specificity_sum =0\n",
    "\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"Train/Validate on fold #{}\".format(i))\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    x_test = unjoin_data(x_test)\n",
    "    x_train = unjoin_data(x_train)\n",
    "    \n",
    "    model.load_weights(\"training_checkpoint.h5\")\n",
    "    out = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=0,\n",
    "              validation_data=(x_test, y_test))\n",
    "    train_acc = out.history[\"accuracy\"][-1]\n",
    "    train_acc_sum += train_acc\n",
    "    val_acc = out.history[\"val_accuracy\"][-1]\n",
    "    val_acc_sum += val_acc\n",
    "    print(\"Training accurracy: {}\".format(train_acc))\n",
    "    print(\"Validation accurracy: {}\".format(val_acc))\n",
    "    \n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = (predictions > THRESHOLD)\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    print(report)\n",
    "    precision = report[\"1\"]['precision']\n",
    "    precision_sum += precision\n",
    "    recall = report[\"1\"]['recall']\n",
    "    recall_sum += recall\n",
    "    f1 = report[\"1\"]['f1']\n",
    "    f1_sum += f1\n",
    "    sensitivity =  report[\"1\"]['recall']\n",
    "    sensitivity_sum += sensitivity\n",
    "    specificity = report[\"0\"]['recall']\n",
    "    specificity_sum += specificity\n",
    "    \n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1 Score: {}\".format(f1))\n",
    "    print(\"Sensitivity: {}\".format(sensitivity))\n",
    "    print(\"Specificity: {}\".format(specificity))\n",
    "    \n",
    "    print()\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.924658477306366\n",
      "Test accuracy: 0.6666667\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.7368421052631579, 'recall': 1.0, 'f1-score': 0.8484848484848484, 'support': 14}, '1': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 10}, 'accuracy': 0.7916666666666666, 'macro avg': {'precision': 0.868421052631579, 'recall': 0.75, 'f1-score': 0.7575757575757576, 'support': 24}, 'weighted avg': {'precision': 0.8464912280701755, 'recall': 0.7916666666666666, 'f1-score': 0.7727272727272726, 'support': 24}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "THRESHOLD = 0.5\n",
    "predictions = model.predict(x_test)\n",
    "predictions = (predictions > THRESHOLD)\n",
    "print(classification_report(y_test, predictions, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False] False\n",
      "[False] False\n",
      "[False] False\n",
      "[False] False\n",
      "[False] False\n",
      "[False] False\n",
      "[False] False\n",
      "[False] True\n",
      "[False] False\n",
      "[False] False\n",
      "[False] True\n",
      "[False] False\n",
      "[False] True\n",
      "[False] False\n",
      "[False] True\n",
      "[False] True\n",
      "[ True] True\n",
      "[False] True\n",
      "[False] True\n",
      "[False] False\n",
      "[False] False\n",
      "[ True] True\n",
      "[False] False\n",
      "[False] True\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "predictions = (predictions > THRESHOLD)\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(pred, bool(y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0]\n",
      " [ 8  2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, (predictions> THRESHOLD))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03506032 0.03605282 0.04228404 0.04486483 0.04895163 0.05007604\n",
      " 0.05525571 0.06735599 0.07177401 0.08200362 0.08577213 0.09934831\n",
      " 0.10985824 0.13300881 0.14835835 0.15615419 0.1741046  0.26006117\n",
      " 0.6918818  0.866511  ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, predictions)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
